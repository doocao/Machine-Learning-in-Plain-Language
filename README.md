# Machine-Learning-in-Plain-Language
零基础读懂机器学习
（清了清嗓子，走到白板前）
最小化这个成本函数 J：

J(β)=1/2m ∑_(i=1)^m▒( y_i-(β_0+β_1 x_i+...+β_n x_ni))^2
我们有两种方法，一种是像“梯度下降”那样迭代地“猜”，另一种就是我们现在要讲的正规方程（Normal Equation），它是一种解析解（Analytical Solution）。
什么叫解析解？通俗点说，就是一步到位，直接用一个数学公式把最优答案算出来，而不是一步步去逼近。
你们都是高年级的学生，微积分和线性代数的基础都很扎实。回想一下，在微积分里，我们如何寻找一个函数的最小值？
（停顿，等待学生回答…）
没错，就是求导，并令导数等于零！
如果是一个变量的函数 f(x)，我们就求 df/dx=0。
现在我们的 J(β) 是一个关于多个变量（β_0,β_1,...,β_n）的函数，所以我们要做的，就是对每一个 β_j 求偏导数，并让所有偏导数都等于零。

∂J/(∂β_0 )=0


∂J/(∂β_1 )=0


...


∂J/(∂β_n )=0
这是一个包含 n+1 个方程的方程组。解出这个方程组，我们就能得到所有最优的 β 值。
 
从微积分到线性代数
一个一个地去求偏导数当然可以，但当特征 n 变得很大时，这个过程会非常繁琐。而这，正是线性代数的威力所在。我们可以把这个问题向量化。
第一步：将成本函数 J 矩阵化
首先，我们把所有数据和参数都写成矩阵和向量的形式：
	特征矩阵 X：这是一个 m×(n+1) 矩阵（m 是样本数，n 是特征数）。为了处理截距项 β_0，我们人为地在第一列添加一个 x_0=1 的特征。

■(X=[■(1&x_11&x_12&…&x_1n@1&x_21&x_22&…&x_2n@⋮&⋮&⋮&⋱&⋮@1&x_m1&x_m2&…&x_mn )] )
	系数向量 β：这是一个 (n+1)×1 的列向量，包含了我们要求解的所有参数。

■(β=[■(β_0@β_1@⋮@β_n )] )
	目标向量 y：这是一个 m×1 的列向量，包含了所有真实的目标值。

■(y=[■(y_1@y_2@⋮@y_m )] )
现在，我们模型的预测向量 y ̂ 可以简洁地表示为：

y ̂=Xβ

（在白板上写下 y ̂=Xβ）
不信你们可以自己乘一下，y ̂ 的第一行就是 β_0⋅1+β_1 x_11+...+β_n x_1n，这正是我们对第一个样本的预测值。
好，那么误差向量 e=y-y ̂ 就是：

e=y-Xβ
我们最初的成本函数（残差平方和）J(β)=∑(y_i-y ̂_i )^2 是什么？不就是误差向量 e 中所有元素的平方和吗？
一个向量中所有元素的平方和，等于这个向量与它自己的点积（Dot Product），也就是 e^T e。
所以，我们的成本函数 J(β) 可以被完美地重写为：（重重地写在白板上）

J(β)=(y-Xβ)^T (y-Xβ)

（注意：我们这里省略了 1/2m，因为它是一个常数，不影响最小值的位置*，只会影响最小值的值，去掉它能让求导更干净。）
第二步：对矩阵化的成本函数 J 求导
我们要找到让 J(β) 最小的 β。我们需要计算 J 对向量 β 的梯度（Gradient），记为 ∇_β J(β)，并令它等于零向量。
我们先展开 J(β)：

█(J(β)&=(y^T-(Xβ)^T)(y-Xβ)@&=(y^T-β^T X^T)(y-Xβ)@&=y^T y-y^T Xβ-β^T X^T y+β^T X^T Xβ)

这里我们要用到一个技巧：y^T Xβ 是一个 1×m⋅m×(n+1)⋅(n+1)×1 的运算，结果是一个 1×1 矩阵，也就是一个标量（scalar）。对于一个标量 a，a=a^T。
所以，(y^T Xβ)^T=β^T X^T (y^T )^T=β^T X^T y。
这两个中间项是相等的！
因此，成本函数简化为：

J(β)=y^T y-2β^T X^T y+β^T X^T Xβ
现在，我们动用矩阵微积分（Matrix Calculus）的知识来求梯度 ∇_β J(β)。你们可以把这当成一组求导规则来记：
	y^T y 不含 β，所以它对 β 的导数是 0。
	∇_β (β^T A)=∇_β (A^T β)=A。这里 A=2X^T y。所以 ∇_β (2β^T X^T y)=2X^T y。（类比于 d/dx(ax)=a）
	∇_β (β^T Aβ)=2Aβ（当 A 是对称矩阵时）。我们的 X^T X 是对称矩阵吗？是的，(X^T X)^T=X^T (X^T )^T=X^T X，它永远是对称的。所以 ∇_β (β^T X^T Xβ)=2(X^T X)β。（类比于 d/dx(ax^2)=2ax）
把这三项合起来，我们得到 J(β) 的梯度：

∇_β J(β)=0-2X^T y+2(X^T X)β
第三步：令梯度为零，求解 β
我们找到了山谷的最低点，在那个点，所有方向的斜率（梯度）都为零。

∇_β J(β)=0 ⃗


-2X^T y+2(X^T X)β=0


2(X^T X)β=2X^T y


(X^T X)β=X^T y
（在白板上框出这个公式）
这个方程 (X^T X)β=X^T y 本身，就是正规方程。
它是一个 Aβ=b 形式的线性方程组，其中 A=X^T X 是一个 (n+1)×(n+1) 的方阵， b=X^T y 是一个 (n+1)×1 的向量。我们的目标是解出 β。
怎么解？只要 A（也就是 X^T X）是可逆的（Invertible），我们就可以在方程两边同时左乘 A 的逆矩阵 A^(-1)：

(X^T X)^(-1) (X^T X)β=(X^T X)^(-1) X^T y


I⋅β=(X^T X)^(-1) X^T y
最终，我们得到了这个梦寐以求的、一步到位的解析解公式：

β ̂=(X^T X)^(-1) X^T y
（在白板上用彩色粉笔写下最终公式）
 
总结：正规方程的优缺点
优点：
1. 一步到位：不需要迭代，直接给出精确的最优解（理论上）。
2. 无需调参：你们看，这个公式里没有任何需要我们手动调整的“学习率 α”，这比梯度下降省心。
缺点（非常重要）：
1. 计算复杂度：这个公式的计算瓶颈在哪里？（指向 (X^T X)^(-1)）在这里！求一个 (n+1)×(n+1) 矩阵的逆，其计算复杂度大约是 O(n^3)，这里的 n 是特征的数量。
2. 可扩展性差：如果你的模型有1000个特征（在AI领域很常见），1000^3 就是10亿次运算。如果你的特征是10万个（比如图像处理），100,000^3=10^15，你的电脑会直接罢工。相比之下，梯度下降每一步的计算量大致是 O(m⋅n)，在 n 很大时，梯度下降反而更快。
3. 矩阵必须可逆：如果 X^T X 是一个奇异矩阵（Singular Matrix），也就是它不可逆（行列式为0），那这个公式就没法用了。
* 什么时候会不可逆？最常见的就是多重共线性（Multicollinearity）：你的特征之间不是线性独立的。比如，你同时用了“房屋面积（平方米）”和“房屋面积（平方英尺）”作为两个特征，它们俩是完全线性相关的。
* 还有一种情况是 m<n，即你的样本量 m 甚至少于你的特征数 n。
* （补充）：在这种情况下，我们通常会使用伪逆（Pseudoinverse），或者使用正则化（比如岭回归），但那就是后话了。
结论：
正规方程为我们提供了坚实的理论基础，让我们从数学上彻底理解了线性回归的解。当特征数量 n 不算太大时（比如 n<1000 或 n<10000，取决于你的机器），它是一个非常优秀、快速且精确的选择。
但当 n 增长到数万、数百万时，我们就必须转向梯度下降这样的迭代优化算法。
明白了吗？好，我们来看一下如何用代码实现这个公式…
<img width="415" height="667" alt="image" src="https://github.com/user-attachments/assets/5c86b9f8-f72f-42eb-baa4-836a1fabaec6" />
